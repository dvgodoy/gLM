{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49d2150e",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "49d2150e",
    "outputId": "d9414938-ccc0-434b-91a4-6b427ab9827b"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting fair-esm==1.0.2\n",
      "  Downloading fair_esm-1.0.2-1-py3-none-any.whl.metadata (33 kB)\n",
      "Downloading fair_esm-1.0.2-1-py3-none-any.whl (76 kB)\n",
      "\u001b[?25l   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/76.4 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m76.4/76.4 kB\u001b[0m \u001b[31m6.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hInstalling collected packages: fair-esm\n",
      "Successfully installed fair-esm-1.0.2\n"
     ]
    }
   ],
   "source": [
    "!pip install fair-esm==1.0.2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b0f3d9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "from transformers import AutoModelForMaskedLM, AutoTokenizer, RobertaConfig\n",
    "from glm_dataset import FastaBatchedContigDataset, get_collate_fn, contig_collate_fn, ContigDataset\n",
    "from glm_model import gLM\n",
    "from glm_utils import save_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "57e99d64",
   "metadata": {
    "id": "57e99d64"
   },
   "outputs": [],
   "source": [
    "model_id = 'facebook/esm2_t33_650M_UR50D'\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_id)\n",
    "fasta_file = '../data/example_data/inference_example/test.fa'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b139948d",
   "metadata": {
    "id": "b139948d"
   },
   "outputs": [],
   "source": [
    "MAX_LEN = 12290\n",
    "\n",
    "cds = FastaBatchedContigDataset.from_file(fasta_file)\n",
    "cds.from_contig_file('../data/example_data/inference_example/contig_to_prots.tsv')\n",
    "batches = cds.get_batch_indices(MAX_LEN, extra_toks_per_seq=1)\n",
    "\n",
    "data_loader = DataLoader(\n",
    "    cds,\n",
    "    collate_fn=get_collate_fn(tokenizer),\n",
    "    batch_sampler=batches\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "258b0024",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch = next(iter(data_loader))\n",
    "batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a785e8f1",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "a785e8f1",
    "outputId": "dbc3ccbd-1349-4185-ff37-3afd9f56e176"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at facebook/esm2_t33_650M_UR50D were not used when initializing EsmForMaskedLM: ['esm.embeddings.position_embeddings.weight']\n",
      "- This IS expected if you are initializing EsmForMaskedLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing EsmForMaskedLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "EsmForMaskedLM(\n",
       "  (esm): EsmModel(\n",
       "    (embeddings): EsmEmbeddings(\n",
       "      (word_embeddings): Embedding(33, 1280, padding_idx=1)\n",
       "      (dropout): Dropout(p=0.0, inplace=False)\n",
       "    )\n",
       "    (encoder): EsmEncoder(\n",
       "      (layer): ModuleList(\n",
       "        (0-32): 33 x EsmLayer(\n",
       "          (attention): EsmAttention(\n",
       "            (self): EsmSelfAttention(\n",
       "              (query): Linear(in_features=1280, out_features=1280, bias=True)\n",
       "              (key): Linear(in_features=1280, out_features=1280, bias=True)\n",
       "              (value): Linear(in_features=1280, out_features=1280, bias=True)\n",
       "              (dropout): Dropout(p=0.0, inplace=False)\n",
       "              (rotary_embeddings): RotaryEmbedding()\n",
       "            )\n",
       "            (output): EsmSelfOutput(\n",
       "              (dense): Linear(in_features=1280, out_features=1280, bias=True)\n",
       "              (dropout): Dropout(p=0.0, inplace=False)\n",
       "            )\n",
       "            (LayerNorm): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)\n",
       "          )\n",
       "          (intermediate): EsmIntermediate(\n",
       "            (dense): Linear(in_features=1280, out_features=5120, bias=True)\n",
       "          )\n",
       "          (output): EsmOutput(\n",
       "            (dense): Linear(in_features=5120, out_features=1280, bias=True)\n",
       "            (dropout): Dropout(p=0.0, inplace=False)\n",
       "          )\n",
       "          (LayerNorm): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "      )\n",
       "      (emb_layer_norm_after): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)\n",
       "    )\n",
       "    (contact_head): EsmContactPredictionHead(\n",
       "      (regression): Linear(in_features=660, out_features=1, bias=True)\n",
       "      (activation): Sigmoid()\n",
       "    )\n",
       "  )\n",
       "  (lm_head): EsmLMHead(\n",
       "    (dense): Linear(in_features=1280, out_features=1280, bias=True)\n",
       "    (layer_norm): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)\n",
       "    (decoder): Linear(in_features=1280, out_features=33, bias=False)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device = 'cuda:0' if torch.cuda.is_available() else 'cpu'\n",
    "esm_model = AutoModelForMaskedLM.from_pretrained(model_id, device_map=device)\n",
    "esm_model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f78070c7",
   "metadata": {
    "id": "f78070c7"
   },
   "outputs": [],
   "source": [
    "def get_plm_embs(esm_model, data_loader):\n",
    "    plm_embs = []\n",
    "\n",
    "    torch.cuda.empty_cache()\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for batch in data_loader:\n",
    "            lens = batch['attention_mask'].sum(dim=1)-2 # removes special tokens from count\n",
    "\n",
    "            batch['input_ids'] = batch['input_ids'].to(device)\n",
    "            batch['attention_mask'] = batch['attention_mask'].to(device)\n",
    "            output = esm_model(input_ids=batch['input_ids'],\n",
    "                              attention_mask=batch['attention_mask'],\n",
    "                              output_hidden_states=True)\n",
    "            states = output['hidden_states'][-1].detach().cpu()\n",
    "            del output\n",
    "            torch.cuda.empty_cache()\n",
    "\n",
    "            for pid, ori, seq_size, state in zip(batch['prot_ids'], batch['prot_oris'], lens, states):\n",
    "                truncate_len = min(MAX_LEN-2, seq_size)\n",
    "                plm_embs.append((pid, ori, state[1:truncate_len+1].mean(0).detach().cpu()))\n",
    "                \n",
    "    return plm_embs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "601cfc05",
   "metadata": {
    "id": "601cfc05"
   },
   "outputs": [],
   "source": [
    "plm_embs = get_plm_embs(esm_model, data_loader)\n",
    "nds = ContigDataset(cds, plm_embs, './preproc/norm_factors.pt', './preproc/pca_parms.pt')\n",
    "ndl = DataLoader(nds, batch_size=1, collate_fn=contig_collate_fn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16869066",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch = next(iter(ndl))\n",
    "batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "0fca79af",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "0fca79af",
    "outputId": "baa0e4a8-df2c-4bdb-a264-30d7742c7105"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--2025-06-23 18:37:50--  https://zenodo.org/record/7855545/files/glm.bin\n",
      "Resolving zenodo.org (zenodo.org)... 188.185.48.194, 188.185.43.25, 188.185.45.92, ...\n",
      "Connecting to zenodo.org (zenodo.org)|188.185.48.194|:443... connected.\n",
      "HTTP request sent, awaiting response... 301 MOVED PERMANENTLY\n",
      "Location: /records/7855545/files/glm.bin [following]\n",
      "--2025-06-23 18:37:50--  https://zenodo.org/records/7855545/files/glm.bin\n",
      "Reusing existing connection to zenodo.org:443.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 3819298935 (3.6G) [application/octet-stream]\n",
      "Saving to: ‘glm.bin’\n",
      "\n",
      "glm.bin             100%[===================>]   3.56G  18.3MB/s    in 3m 8s   \n",
      "\n",
      "2025-06-23 18:40:58 (19.4 MB/s) - ‘glm.bin’ saved [3819298935/3819298935]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "!mkdir model\n",
    "!wget https://zenodo.org/record/7855545/files/glm.bin --output-document=./model/glm.bin"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "1ff8662a",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "1ff8662a",
    "outputId": "23b6fe3d-bb25-43b1-bce6-4b9f255f0a80"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "gLM(\n",
       "  (embeddings): RobertaEmbeddings(\n",
       "    (word_embeddings): Embedding(30522, 1280, padding_idx=1)\n",
       "    (position_embeddings): Embedding(512, 1280, padding_idx=1)\n",
       "    (token_type_embeddings): Embedding(1, 1280)\n",
       "    (LayerNorm): LayerNorm((1280,), eps=1e-12, elementwise_affine=True)\n",
       "    (dropout): Dropout(p=0.1, inplace=False)\n",
       "  )\n",
       "  (encoder): RobertaEncoder(\n",
       "    (layer): ModuleList(\n",
       "      (0-18): 19 x RobertaLayer(\n",
       "        (attention): RobertaAttention(\n",
       "          (self): RobertaSelfAttention(\n",
       "            (query): Linear(in_features=1280, out_features=1280, bias=True)\n",
       "            (key): Linear(in_features=1280, out_features=1280, bias=True)\n",
       "            (value): Linear(in_features=1280, out_features=1280, bias=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "            (distance_embedding): Embedding(1023, 128)\n",
       "          )\n",
       "          (output): RobertaSelfOutput(\n",
       "            (dense): Linear(in_features=1280, out_features=1280, bias=True)\n",
       "            (LayerNorm): LayerNorm((1280,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (intermediate): RobertaIntermediate(\n",
       "          (dense): Linear(in_features=1280, out_features=3072, bias=True)\n",
       "          (intermediate_act_fn): GELUActivation()\n",
       "        )\n",
       "        (output): RobertaOutput(\n",
       "          (dense): Linear(in_features=3072, out_features=1280, bias=True)\n",
       "          (LayerNorm): LayerNorm((1280,), eps=1e-12, elementwise_affine=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (pooler): RobertaPooler(\n",
       "    (dense): Linear(in_features=1280, out_features=1280, bias=True)\n",
       "    (activation): Tanh()\n",
       "  )\n",
       "  (roberta): gLM_base(\n",
       "    (embeddings): RobertaEmbeddings(\n",
       "      (word_embeddings): Embedding(30522, 1280, padding_idx=1)\n",
       "      (position_embeddings): Embedding(512, 1280, padding_idx=1)\n",
       "      (token_type_embeddings): Embedding(1, 1280)\n",
       "      (LayerNorm): LayerNorm((1280,), eps=1e-12, elementwise_affine=True)\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (encoder): RobertaEncoder(\n",
       "      (layer): ModuleList(\n",
       "        (0-18): 19 x RobertaLayer(\n",
       "          (attention): RobertaAttention(\n",
       "            (self): RobertaSelfAttention(\n",
       "              (query): Linear(in_features=1280, out_features=1280, bias=True)\n",
       "              (key): Linear(in_features=1280, out_features=1280, bias=True)\n",
       "              (value): Linear(in_features=1280, out_features=1280, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "              (distance_embedding): Embedding(1023, 128)\n",
       "            )\n",
       "            (output): RobertaSelfOutput(\n",
       "              (dense): Linear(in_features=1280, out_features=1280, bias=True)\n",
       "              (LayerNorm): LayerNorm((1280,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): RobertaIntermediate(\n",
       "            (dense): Linear(in_features=1280, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): RobertaOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=1280, bias=True)\n",
       "            (LayerNorm): LayerNorm((1280,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (pooler): RobertaPooler(\n",
       "      (dense): Linear(in_features=1280, out_features=1280, bias=True)\n",
       "      (activation): Tanh()\n",
       "    )\n",
       "    (roberta): RobertaModel(\n",
       "      (embeddings): RobertaEmbeddings(\n",
       "        (word_embeddings): Embedding(30522, 1280, padding_idx=1)\n",
       "        (position_embeddings): Embedding(512, 1280, padding_idx=1)\n",
       "        (token_type_embeddings): Embedding(1, 1280)\n",
       "        (LayerNorm): LayerNorm((1280,), eps=1e-12, elementwise_affine=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (encoder): RobertaEncoder(\n",
       "        (layer): ModuleList(\n",
       "          (0-18): 19 x RobertaLayer(\n",
       "            (attention): RobertaAttention(\n",
       "              (self): RobertaSelfAttention(\n",
       "                (query): Linear(in_features=1280, out_features=1280, bias=True)\n",
       "                (key): Linear(in_features=1280, out_features=1280, bias=True)\n",
       "                (value): Linear(in_features=1280, out_features=1280, bias=True)\n",
       "                (dropout): Dropout(p=0.1, inplace=False)\n",
       "                (distance_embedding): Embedding(1023, 128)\n",
       "              )\n",
       "              (output): RobertaSelfOutput(\n",
       "                (dense): Linear(in_features=1280, out_features=1280, bias=True)\n",
       "                (LayerNorm): LayerNorm((1280,), eps=1e-12, elementwise_affine=True)\n",
       "                (dropout): Dropout(p=0.1, inplace=False)\n",
       "              )\n",
       "            )\n",
       "            (intermediate): RobertaIntermediate(\n",
       "              (dense): Linear(in_features=1280, out_features=3072, bias=True)\n",
       "              (intermediate_act_fn): GELUActivation()\n",
       "            )\n",
       "            (output): RobertaOutput(\n",
       "              (dense): Linear(in_features=3072, out_features=1280, bias=True)\n",
       "              (LayerNorm): LayerNorm((1280,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (lm_head): gLMMultiHead(\n",
       "      (dense): Linear(in_features=1280, out_features=400, bias=True)\n",
       "      (dense_prob): Linear(in_features=1280, out_features=4, bias=True)\n",
       "    )\n",
       "    (contact_head): ContactPredictionHead(\n",
       "      (regression): Linear(in_features=190, out_features=1, bias=True)\n",
       "    )\n",
       "  )\n",
       "  (dense): Linear(in_features=1281, out_features=1280, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "HIDDEN_SIZE = 1280\n",
    "HALF = True\n",
    "EMB_DIM = 1281\n",
    "NUM_PC_LABEL = 100\n",
    "\n",
    "num_pred = 4\n",
    "max_seq_length = 30\n",
    "num_attention_heads = 10\n",
    "num_hidden_layers= 19\n",
    "pos_emb = \"relative_key_query\"\n",
    "pred_probs = True\n",
    "\n",
    "config = RobertaConfig(\n",
    "    vocab_size = 30522,\n",
    "    max_position_embedding = max_seq_length,\n",
    "    hidden_size = HIDDEN_SIZE,\n",
    "    num_attention_heads = num_attention_heads,\n",
    "    type_vocab_size = 1,\n",
    "    tie_word_embeddings = False,\n",
    "    num_hidden_layers = num_hidden_layers,\n",
    "    num_pc = NUM_PC_LABEL,\n",
    "    num_pred = num_pred,\n",
    "    predict_probs = pred_probs,\n",
    "    emb_dim = EMB_DIM,\n",
    "    output_attentions = True,\n",
    "    output_hidden_states = True,\n",
    "    position_embedding_type = pos_emb,\n",
    "    attn_implementation = \"eager\"\n",
    ")\n",
    "\n",
    "model_path = './glm.bin'\n",
    "\n",
    "model =  gLM(config)\n",
    "model.load_state_dict(torch.load(model_path, map_location=device),strict=False)\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "15575d56",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_glm_embs(model, data_loader, half_precision=True):\n",
    "    if not torch.cuda.is_available():\n",
    "        half_precision = False\n",
    "\n",
    "    torch.cuda.empty_cache()\n",
    "    scaler = None\n",
    "\n",
    "    with torch.no_grad():\n",
    "        all_outputs = {}\n",
    "        all_batches = {}\n",
    "        for batch in data_loader:\n",
    "            batch['inputs_embeds'] = batch['inputs_embeds'].to(device)\n",
    "            batch['labels'] = batch['labels'].to(device)\n",
    "            batch['attention_mask'] = batch['attention_mask'].to(device)\n",
    "\n",
    "            if half_precision:\n",
    "                scaler = torch.cuda.amp.GradScaler()\n",
    "                with torch.amp.autocast(device_type='cuda', dtype=torch.float16):\n",
    "                    output = model(**batch)\n",
    "            else:\n",
    "                output = model(**batch)\n",
    "\n",
    "            for k in ['logits_all_preds', 'probs', 'last_hidden_state', 'contacts']:\n",
    "                current = all_outputs.get(k, None)\n",
    "                if current is None:\n",
    "                    all_outputs[k] = output[k].detach().cpu()\n",
    "                else:\n",
    "                    all_outputs[k] = torch.cat([all_outputs[k], output[k].detach().cpu()], dim=0)\n",
    "            del output\n",
    "            for k in batch.keys():\n",
    "                current = all_batches.get(k, None)\n",
    "                if current is None:\n",
    "                    all_batches[k] = batch[k].detach().cpu()\n",
    "                else:\n",
    "                    all_batches[k] = torch.cat([all_batches[k], batch[k].detach().cpu()], dim=0)\n",
    "            torch.cuda.empty_cache()\n",
    "            \n",
    "    return all_batches, all_outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "840c8d47",
   "metadata": {
    "id": "840c8d47"
   },
   "outputs": [],
   "source": [
    "all_batches, all_outputs = get_glm_embs(model, ndl, HALF)\n",
    "save_results(all_batches, all_outputs, './results', 'test')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "4pqpBUX0GzWW",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "4pqpBUX0GzWW",
    "outputId": "b5c6e180-99cd-4724-b8df-952d47a20162"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.3616 ,  0.0715 , -0.3362 , ..., -0.2272 ,  0.02887,  0.06335],\n",
       "       [ 0.379  ,  0.0717 , -0.3325 , ..., -0.2264 ,  0.03049,  0.04584],\n",
       "       [ 0.374  ,  0.427  , -0.319  , ..., -0.5903 ,  0.4385 , -0.2443 ],\n",
       "       ...,\n",
       "       [-0.3157 , -0.0497 , -0.5293 , ..., -0.11896,  0.297  ,  0.4146 ],\n",
       "       [-0.9995 , -0.4336 , -0.4648 , ..., -0.2656 ,  0.7754 ,  0.4424 ],\n",
       "       [ 0.3442 ,  0.09143, -0.305  , ..., -0.1823 ,  0.02934,  0.0788 ]],\n",
       "      dtype=float16)"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "glm_embs = torch.load('test_glm_embs.pt', weights_only=False)\n",
    "glm_embs[0]"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
