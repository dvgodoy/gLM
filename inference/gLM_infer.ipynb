{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "Z98cbwtCgbzk",
   "metadata": {
    "id": "Z98cbwtCgbzk"
   },
   "source": [
    "# gLM Inference\n",
    "\n",
    "This notebook was developed to allow for easy inference using the pretrained Genome Language Model (gLM) in Google Colab.\n",
    "\n",
    "***\n",
    "For more details about the model, please check the original paper [_Genomic language model predicts protein co-regulation and function_](https://www.biorxiv.org/content/10.1101/2023.04.07.536042v3) by Yunha Hwang, Andre L. Cornman, Elizabeth H. Kellogg, Sergey Ovchinnikov, Peter R. Girguis.\n",
    "***\n",
    "\n",
    "The inference process follows these steps:\n",
    "- loading a FASTA of proteins\n",
    "- loading a contig (TSV) file with contig-to-protein mappings\n",
    "- embedding proteins (pLM embeddings) using pretrained ESM model\n",
    "- preprocessing pLM embeddings (normalization for inputs, PCA for labels)\n",
    "- embedding proteins contextually using pretrained gLM model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "tD6K4Uy5gfTJ",
   "metadata": {
    "id": "tD6K4Uy5gfTJ"
   },
   "source": [
    "## Setup\n",
    "\n",
    "Apart from Colab's default environment, we only need to install `fair-esm` package."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "49d2150e",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "49d2150e",
    "outputId": "808d8930-1ec5-4455-a4d5-4c13667cb05c"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting fair-esm==1.0.2\n",
      "  Downloading fair_esm-1.0.2-1-py3-none-any.whl.metadata (33 kB)\n",
      "Downloading fair_esm-1.0.2-1-py3-none-any.whl (76 kB)\n",
      "\u001b[?25l   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/76.4 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m76.4/76.4 kB\u001b[0m \u001b[31m5.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hInstalling collected packages: fair-esm\n",
      "Successfully installed fair-esm-1.0.2\n"
     ]
    }
   ],
   "source": [
    "!pip install fair-esm==1.0.2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d754db49",
   "metadata": {},
   "source": [
    "Next, we download custom dataset and model classes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "jsM8am3AdTmP",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "jsM8am3AdTmP",
    "outputId": "74036bd5-5173-4abc-a9ad-4e3da2be17bd"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--2025-06-24 15:01:59--  https://raw.githubusercontent.com/dvgodoy/gLM/refs/heads/main/inference/glm_dataset.py\n",
      "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.108.133, 185.199.109.133, 185.199.110.133, ...\n",
      "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.108.133|:443... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 9940 (9.7K) [text/plain]\n",
      "Saving to: ‘glm_dataset.py’\n",
      "\n",
      "\r",
      "glm_dataset.py        0%[                    ]       0  --.-KB/s               \r",
      "glm_dataset.py      100%[===================>]   9.71K  --.-KB/s    in 0.001s  \n",
      "\n",
      "2025-06-24 15:01:59 (15.6 MB/s) - ‘glm_dataset.py’ saved [9940/9940]\n",
      "\n",
      "--2025-06-24 15:02:00--  https://raw.githubusercontent.com/dvgodoy/gLM/refs/heads/main/inference/glm_model.py\n",
      "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.109.133, 185.199.110.133, 185.199.111.133, ...\n",
      "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.109.133|:443... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 13378 (13K) [text/plain]\n",
      "Saving to: ‘glm_model.py’\n",
      "\n",
      "glm_model.py        100%[===================>]  13.06K  --.-KB/s    in 0.001s  \n",
      "\n",
      "2025-06-24 15:02:00 (18.3 MB/s) - ‘glm_model.py’ saved [13378/13378]\n",
      "\n",
      "--2025-06-24 15:02:00--  https://raw.githubusercontent.com/dvgodoy/gLM/refs/heads/main/inference/glm_utils.py\n",
      "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.110.133, 185.199.108.133, 185.199.109.133, ...\n",
      "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.110.133|:443... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 2122 (2.1K) [text/plain]\n",
      "Saving to: ‘glm_utils.py’\n",
      "\n",
      "glm_utils.py        100%[===================>]   2.07K  --.-KB/s    in 0s      \n",
      "\n",
      "2025-06-24 15:02:00 (31.6 MB/s) - ‘glm_utils.py’ saved [2122/2122]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "!wget https://raw.githubusercontent.com/dvgodoy/gLM/refs/heads/main/inference/glm_dataset.py\n",
    "!wget https://raw.githubusercontent.com/dvgodoy/gLM/refs/heads/main/inference/glm_model.py\n",
    "!wget https://raw.githubusercontent.com/dvgodoy/gLM/refs/heads/main/inference/glm_utils.py"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c38_S4dgi1L",
   "metadata": {
    "id": "3c38_S4dgi1L"
   },
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3b0f3d9b",
   "metadata": {
    "id": "3b0f3d9b"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "from transformers import AutoModelForMaskedLM, AutoTokenizer, RobertaConfig\n",
    "from glm_dataset import FastaBatchedContigDataset, get_collate_fn, contig_collate_fn, ContigDataset\n",
    "from glm_model import gLM\n",
    "from glm_utils import save_results"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "j3dV0HwBgj8B",
   "metadata": {
    "id": "j3dV0HwBgj8B"
   },
   "source": [
    "## Data\n",
    "\n",
    "The first file we'll use is a typical FASTA file containing the protein sequences, such as:\n",
    "\n",
    "```\n",
    ">lcl|NC_000913.3_prot_NP_414542.1_1 [gene=thrL] [locus_tag=b0001] [db_xref=UniProtKB/Swiss-Prot:P0AD86] [protein=thr operon leader peptide] [protein_id=NP_414542.1] [location=190..255] [gbkey=CDS]\n",
    "MKRISTTITTTITITTGNGAG\n",
    "```\n",
    "\n",
    "In the protein above, the we'll be using the header (up to the first space) to identify the protein, e.g. `lcl|NC_000913.3_prot_NP_414542.1_1`.\n",
    "\n",
    "The second file, is a TSV (tab-separated values) file where each line represents a contig, such as:\n",
    "\n",
    "```\n",
    "contig_0\\t+lcl|NC_000913.3_prot_NP_414542.1_1;+lcl|NC_000913.3_prot_NP_414543.1_2;+lcl|NC_000913.3_prot_NP_414544.1_3;+lcl|NC_000913.3_prot_NP_414545.1_4;+lcl|NC_000913.3_prot_NP_414546.1_5;-lcl|NC_000913.3_prot_NP_414547.1_6;-lcl|NC_000913.3_prot_NP_414548.1_7;+lcl|NC_000913.3_prot_NP_414549.1_8;+lcl|NC_000913.3_prot_NP_414550.1_9;-lcl|NC_000913.3_prot_NP_414551.1_10;-lcl|NC_000913.3_prot_NP_414552.1_11;+lcl|NC_000913.3_prot_YP_009518733.1_12;-lcl|NC_000913.3_prot_NP_414554.1_13;+lcl|NC_000913.3_prot_NP_414555.1_14;+lcl|NC_000913.3_prot_NP_414556.1_15;+lcl|NC_000913.3_prot_NP_414557.1_16;-lcl|NC_000913.3_prot_NP_414559.1_17;-lcl|NC_000913.3_prot_YP_025292.1_18;+lcl|NC_000913.3_prot_NP_414560.1_19;+lcl|NC_000913.3_prot_NP_414561.1_20;-lcl|NC_000913.3_prot_NP_414562.1_21;-lcl|NC_000913.3_prot_NP_414563.1_22;-lcl|NC_000913.3_prot_NP_414564.1_23;+lcl|NC_000913.3_prot_NP_414565.1_24;+lcl|NC_000913.3_prot_NP_414566.1_25;+lcl|NC_000913.3_prot_NP_414567.1_26;+lcl|NC_000913.3_prot_NP_414568.1_27;+lcl|NC_000913.3_prot_NP_414569.1_28;+lcl|NC_000913.3_prot_NP_414570.1_29;+lcl|NC_000913.3_prot_NP_414571.1_30\n",
    "```\n",
    "\n",
    "The contig name, `contig_0` in the example above, is the first field, and everything the follow the tab (`\\t`) is a sequence of all proteins in the contig (notice that the name of the first protein matches the first entry of our FASTA file) separated by \"`;`\". Moreover, each protein's ID is preceded by their orientation, either `+` or `-`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ZffWb-4YdysN",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ZffWb-4YdysN",
    "outputId": "51456d74-2948-430e-c9a8-723c89e5ab6d"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--2025-06-24 17:00:25--  https://raw.githubusercontent.com/dvgodoy/gLM/refs/heads/main/data/example_data/inference_example/test.fa\n",
      "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 2606:50c0:8001::154, 2606:50c0:8002::154, 2606:50c0:8000::154, ...\n",
      "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|2606:50c0:8001::154|:443... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 33005 (32K) [text/plain]\n",
      "Saving to: ‘test.fa’\n",
      "\n",
      "test.fa             100%[===================>]  32.23K  --.-KB/s    in 0.003s  \n",
      "\n",
      "2025-06-24 17:00:25 (11.5 MB/s) - ‘test.fa’ saved [33005/33005]\n",
      "\n",
      "--2025-06-24 17:00:25--  https://raw.githubusercontent.com/dvgodoy/gLM/refs/heads/main/data/example_data/inference_example/contig_to_prots.tsv\n",
      "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 2606:50c0:8001::154, 2606:50c0:8002::154, 2606:50c0:8000::154, ...\n",
      "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|2606:50c0:8001::154|:443... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 2237 (2.2K) [text/plain]\n",
      "Saving to: ‘contig_to_prots.tsv’\n",
      "\n",
      "contig_to_prots.tsv 100%[===================>]   2.18K  --.-KB/s    in 0s      \n",
      "\n",
      "2025-06-24 17:00:26 (9.65 MB/s) - ‘contig_to_prots.tsv’ saved [2237/2237]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "!wget https://raw.githubusercontent.com/dvgodoy/gLM/refs/heads/main/data/example_data/inference_example/test.fa\n",
    "!wget https://raw.githubusercontent.com/dvgodoy/gLM/refs/heads/main/data/example_data/inference_example/contig_to_prots.tsv"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "K6ugBzb8g72L",
   "metadata": {
    "id": "K6ugBzb8g72L"
   },
   "source": [
    "## Dataset for Protein Language Model (pLM) Embeddings\n",
    "\n",
    "We'll be using a customizer version of ESM's `FastaBatchedDataset` to include contig information. Our class is named `FastaBatchedContigDataset` and it allows for loading the contigs file using the `from_contig_file()` method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "57e99d64",
   "metadata": {
    "id": "57e99d64"
   },
   "outputs": [],
   "source": [
    "fasta_file = 'test.fa'\n",
    "contigs_file = 'contig_to_prots.tsv'\n",
    "MAX_LEN = 12290\n",
    "\n",
    "# Loads FASTA file into customized FastaBatchedDataset\n",
    "cds = FastaBatchedContigDataset.from_file(fasta_file)\n",
    "\n",
    "# Imports contig to proteins mapping\n",
    "# It will assign sequential IDs to proteins\n",
    "# And then map the proteins in the contigs to their corresponding IDs, \n",
    "# while also updating origin (+/-) information for each protein\n",
    "cds.from_contig_file(contigs_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a0a7e3d",
   "metadata": {},
   "source": [
    "### Protein Mappings\n",
    "\n",
    "Our custom class has a few new attributes, such as `prot2id`, `id2prot`, and `prot_oris`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "5400d2b1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['lcl|NC_000913.3_prot_NP_414542.1_1',\n",
       " 'lcl|NC_000913.3_prot_NP_414543.1_2',\n",
       " 'lcl|NC_000913.3_prot_NP_414544.1_3']"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(cds.prot2id.keys())[:3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "9dadea33",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0, 'lcl|NC_000913.3_prot_NP_414542.1_1', '+')"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(cds.prot2id['lcl|NC_000913.3_prot_NP_414542.1_1'],\n",
    " cds.id2prot[0],\n",
    " cds.prot_oris['lcl|NC_000913.3_prot_NP_414542.1_1'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3686508",
   "metadata": {},
   "source": [
    "### Contig Mappings\n",
    "\n",
    "It also has a `contigs` attribute, a dictionary of all contigs loaded, where their corresponding values are a list of the sequential IDs of their corresponding proteins."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "3747f638",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['contig_0', 'contig_1'])"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cds.contigs.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "23a93617",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29]\n"
     ]
    }
   ],
   "source": [
    "print(cds.contigs['contig_0'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7d33cb8",
   "metadata": {},
   "source": [
    "## Tokenizer\n",
    "\n",
    "The ESM model doesn't work with raw sequences, but tokenized ones. The job of the tokenizer is to convert each symbol into its corresponding ID. The vocabulary is only 33 tokens long, including all amino acids and a few extra special tokens."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "da413e32",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['<cls>', '<pad>', '<eos>', '<unk>', 'L', 'A', 'G', 'V', 'S', 'E', 'R', 'T', 'I', 'D', 'P', 'K', 'Q', 'N', 'F', 'Y', 'M', 'H', 'W', 'C', 'X', 'B', 'U', 'Z', 'O', '.', '-', '<null_1>', '<mask>']\n"
     ]
    }
   ],
   "source": [
    "# Loads ESM tokenizer to convert proteins into IDs\n",
    "# The tokenizer will pad the shortest sequences with 0s\n",
    "model_id = 'facebook/esm2_t33_650M_UR50D'\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_id)\n",
    "\n",
    "print(tokenizer.all_tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "316f7610",
   "metadata": {},
   "source": [
    "## Mini-Batches\n",
    "\n",
    "In order to send data to the ESM model, we'll group proteins in mini-batches, so that they do not exceed the maximum length configured. Also, we're using a custom collate function that uses the tokenizer to convert the sequences into their IDs, pad them to match their lengths, and include additional information such as the proteins orientation and label."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "b139948d",
   "metadata": {
    "id": "b139948d"
   },
   "outputs": [],
   "source": [
    "# Determines which elements from FASTA file are contained in each batch\n",
    "batches = cds.get_batch_indices(MAX_LEN, extra_toks_per_seq=1)\n",
    "\n",
    "# Uses custom collate_fn to produce mini-batches containing\n",
    "# input_ids, attention_masks, prot_ids, prot_oris, labels\n",
    "data_loader = DataLoader(\n",
    "    cds,\n",
    "    collate_fn=get_collate_fn(tokenizer),\n",
    "    batch_sampler=batches\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "258b0024",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "258b0024",
    "outputId": "fa9d40b4-6490-4688-db23-d80ffc4bf8c1"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': tensor([[ 0, 20, 15,  ...,  1,  1,  1],\n",
       "        [ 0, 20, 15,  ...,  1,  1,  1],\n",
       "        [ 0, 20, 17,  ...,  1,  1,  1],\n",
       "        ...,\n",
       "        [ 0, 20, 17,  ...,  1,  1,  1],\n",
       "        [ 0, 20, 16,  ..., 13,  2,  1],\n",
       "        [ 0, 20, 11,  ...,  4,  4,  2]]), 'attention_mask': tensor([[1, 1, 1,  ..., 0, 0, 0],\n",
       "        [1, 1, 1,  ..., 0, 0, 0],\n",
       "        [1, 1, 1,  ..., 0, 0, 0],\n",
       "        ...,\n",
       "        [1, 1, 1,  ..., 0, 0, 0],\n",
       "        [1, 1, 1,  ..., 1, 1, 0],\n",
       "        [1, 1, 1,  ..., 1, 1, 1]]), 'prot_ids': (0, 17, 56, 16, 23, 22, 21, 43, 4, 49, 33, 12, 27, 47, 11, 26, 20, 45, 9, 8, 34, 55, 57, 10, 40, 5, 35, 54, 30, 50, 48, 19, 29, 2, 24, 41, 28, 7), 'prot_oris': ('+', '-', '+', '-', '+', '-', '-', '+', '+', '-', '+', '-', '+', '+', '+', '+', '-', '+', '-', '+', '-', '+', '-', '-', '+', '-', '-', '+', '+', '-', '-', '+', '+', '+', '+', '+', '+', '+'), 'labels': ('lcl|NC_000913.3_prot_NP_414542.1_1 [gene=thrL] [locus_tag=b0001] [db_xref=UniProtKB/Swiss-Prot:P0AD86] [protein=thr operon leader peptide] [protein_id=NP_414542.1] [location=190..255] [gbkey=CDS]', 'lcl|NC_000913.3_prot_YP_025292.1_18 [gene=hokC] [locus_tag=b4412] [db_xref=UniProtKB/Swiss-Prot:P0ACG4] [protein=protein HokC] [protein_id=YP_025292.1] [location=complement(16751..16903)] [gbkey=CDS]', 'lcl|NC_000913.3_prot_YP_009518735.1_57 [gene=yabQ] [locus_tag=b0057] [db_xref=UniProtKB/Swiss-Prot:P39221] [protein=protein YabQ] [protein_id=YP_009518735.1] [location=59121..59279] [gbkey=CDS]', 'lcl|NC_000913.3_prot_NP_414559.1_17 [gene=mokC] [locus_tag=b0018] [db_xref=UniProtKB/Swiss-Prot:P33236] [protein=regulatory protein MokC] [protein_id=NP_414559.1] [location=complement(16751..16960)] [gbkey=CDS]', 'lcl|NC_000913.3_prot_NP_414565.1_24 [gene=yaaY] [locus_tag=b0024] [db_xref=UniProtKB/Swiss-Prot:P75620] [protein=DUF2575 domain-containing protein YaaY] [protein_id=NP_414565.1] [location=21181..21399] [gbkey=CDS]', 'lcl|NC_000913.3_prot_NP_414564.1_23 [gene=rpsT] [locus_tag=b0023] [db_xref=UniProtKB/Swiss-Prot:P0A7U7] [protein=30S ribosomal subunit protein S20] [protein_id=NP_414564.1] [location=complement(20815..21078)] [gbkey=CDS]', 'lcl|NC_000913.3_prot_NP_414563.1_22 [gene=insA1] [locus_tag=b0022] [db_xref=UniProtKB/Swiss-Prot:P0CF07] [protein=IS1 family protein InsA] [protein_id=NP_414563.1] [location=complement(20233..20508)] [gbkey=CDS]', 'lcl|NC_000913.3_prot_NP_414586.1_44 [gene=fixX] [locus_tag=b0044] [db_xref=UniProtKB/Swiss-Prot:P68646] [protein=putative ferredoxin FixX] [protein_id=NP_414586.1] [location=45463..45750] [gbkey=CDS]', 'lcl|NC_000913.3_prot_NP_414546.1_5 [gene=yaaX] [locus_tag=b0005] [db_xref=UniProtKB/Swiss-Prot:P75616] [protein=DUF2502 domain-containing protein YaaX] [protein_id=NP_414546.1] [location=5234..5530] [gbkey=CDS]', 'lcl|NC_000913.3_prot_NP_414592.1_50 [gene=apaG] [locus_tag=b0050] [db_xref=UniProtKB/Swiss-Prot:P62672] [protein=DUF525 domain-containing protein ApaG] [protein_id=NP_414592.1] [location=complement(51229..51606)] [gbkey=CDS]', 'lcl|NC_000913.3_prot_NP_414576.4_34 [gene=caiF] [locus_tag=b0034] [db_xref=UniProtKB/Swiss-Prot:P0AE58] [protein=DNA-binding transcriptional activator CaiF] [protein_id=NP_414576.4] [location=34300..34695] [gbkey=CDS]', 'lcl|NC_000913.3_prot_NP_414554.1_13 [gene=yaaI] [locus_tag=b0013] [db_xref=UniProtKB/Swiss-Prot:P28696] [protein=DUF2541 domain-containing protein YaaI] [protein_id=NP_414554.1] [location=complement(11382..11786)] [gbkey=CDS]', 'lcl|NC_000913.3_prot_NP_414569.1_28 [gene=fkpB] [locus_tag=b0028] [db_xref=UniProtKB/Swiss-Prot:P0AEM0] [protein=peptidyl-prolyl cis-trans isomerase FkpB] [protein_id=NP_414569.1] [location=25826..26275] [gbkey=CDS]', 'lcl|NC_000913.3_prot_NP_414590.1_48 [gene=folA] [locus_tag=b0048] [db_xref=UniProtKB/Swiss-Prot:P0ABQ4] [protein=dihydrofolate reductase] [protein_id=NP_414590.1] [location=49823..50302] [gbkey=CDS]', 'lcl|NC_000913.3_prot_YP_009518733.1_12 [gene=mbiA] [locus_tag=b0012] [db_xref=UniProtKB/Swiss-Prot:P28697] [protein=uncharacterized protein MbiA] [protein_id=YP_009518733.1] [location=10830..11315] [gbkey=CDS]', 'lcl|NC_000913.3_prot_NP_414568.1_27 [gene=lspA] [locus_tag=b0027] [db_xref=UniProtKB/Swiss-Prot:P00804] [protein=lipoprotein signal peptidase] [protein_id=NP_414568.1] [location=25207..25701] [gbkey=CDS]', 'lcl|NC_000913.3_prot_NP_414562.1_21 [gene=insB1] [locus_tag=b0021] [db_xref=UniProtKB/Swiss-Prot:P0CF25] [protein=IS1 family protein InsB] [protein_id=NP_414562.1] [location=complement(19811..20314)] [gbkey=CDS]', 'lcl|NC_000913.3_prot_NP_414588.1_46 [gene=kefF] [locus_tag=b0046] [db_xref=UniProtKB/Swiss-Prot:P0A754] [protein=regulator of KefC-mediated potassium transport and quinone oxidoreductase] [protein_id=NP_414588.1] [location=47246..47776] [gbkey=CDS]', 'lcl|NC_000913.3_prot_NP_414551.1_10 [gene=satP] [locus_tag=b0010] [db_xref=UniProtKB/Swiss-Prot:P0AC98] [protein=acetate/succinate:H(+) symporter] [protein_id=NP_414551.1] [location=complement(9928..10494)] [gbkey=CDS]', 'lcl|NC_000913.3_prot_NP_414550.1_9 [gene=mog] [locus_tag=b0009] [db_xref=UniProtKB/Swiss-Prot:P0AF03] [protein=molybdopterin adenylyltransferase] [protein_id=NP_414550.1] [location=9306..9893] [gbkey=CDS]', 'lcl|NC_000913.3_prot_NP_414577.2_35 [gene=caiE] [locus_tag=b0035] [db_xref=UniProtKB/Swiss-Prot:P39206] [protein=putative transferase CaiE] [protein_id=NP_414577.2] [location=complement(34781..35371)] [gbkey=CDS]', 'lcl|NC_000913.3_prot_YP_009518734.1_56 [gene=yabP] [locus_tag=b0056] [db_xref=UniProtKB/Swiss-Prot:P39220] [protein=putative uncharacterized protein YabP] [protein_id=YP_009518734.1] [location=58474..59124] [gbkey=CDS]', 'lcl|NC_000913.3_prot_NP_414600.1_58 [gene=rluA] [locus_tag=b0058] [db_xref=UniProtKB/Swiss-Prot:P0AA37] [protein=23S rRNA pseudouridine(746) and tRNA pseudouridine(32) synthase] [protein_id=NP_414600.1] [location=complement(59687..60346)] [gbkey=CDS]', 'lcl|NC_000913.3_prot_NP_414552.1_11 [gene=yaaW] [locus_tag=b0011] [db_xref=UniProtKB/Swiss-Prot:P75617] [protein=putative enzyme-specific chaperone YaaW] [protein_id=NP_414552.1] [location=complement(10643..11356)] [gbkey=CDS]', 'lcl|NC_000913.3_prot_NP_414583.2_41 [gene=fixA] [locus_tag=b0041] [db_xref=UniProtKB/Swiss-Prot:P60566] [protein=putative electron transfer flavoprotein FixA] [protein_id=NP_414583.2] [location=42403..43173] [gbkey=CDS]', 'lcl|NC_000913.3_prot_NP_414547.1_6 [gene=yaaA] [locus_tag=b0006] [db_xref=UniProtKB/Swiss-Prot:P0A8I3] [protein=DNA binding and peroxide stress response protein YaaA] [protein_id=NP_414547.1] [location=complement(5683..6459)] [gbkey=CDS]', 'lcl|NC_000913.3_prot_NP_414578.2_36 [gene=caiD] [locus_tag=b0036] [db_xref=UniProtKB/Swiss-Prot:P31551] [protein=crotonobetainyl-CoA hydratase] [protein_id=NP_414578.2] [location=complement(35377..36162)] [gbkey=CDS]', 'lcl|NC_000913.3_prot_NP_414597.1_55 [gene=djlA] [locus_tag=b0055] [db_xref=UniProtKB/Swiss-Prot:P31680] [protein=co-chaperone protein DjlA] [protein_id=NP_414597.1] [location=57364..58179] [gbkey=CDS]', 'lcl|NC_000913.3_prot_NP_414572.1_31 [gene=dapB] [locus_tag=b0031] [db_xref=UniProtKB/Swiss-Prot:P04036] [protein=4-hydroxy-tetrahydrodipicolinate reductase] [protein_id=NP_414572.1] [location=28374..29195] [gbkey=CDS]', 'lcl|NC_000913.3_prot_NP_414593.1_51 [gene=rsmA] [locus_tag=b0051] [db_xref=UniProtKB/Swiss-Prot:P06992] [protein=16S rRNA m(6)2A1518,m(6)2A1519 dimethyltransferase] [protein_id=NP_414593.1] [location=complement(51609..52430)] [gbkey=CDS]', 'lcl|NC_000913.3_prot_NP_414591.1_49 [gene=apaH] [locus_tag=b0049] [db_xref=UniProtKB/Swiss-Prot:P05637] [protein=diadenosine tetraphosphatase] [protein_id=NP_414591.1] [location=complement(50380..51222)] [gbkey=CDS]', 'lcl|NC_000913.3_prot_NP_414561.1_20 [gene=nhaR] [locus_tag=b0020] [db_xref=UniProtKB/Swiss-Prot:P0A9G2] [protein=DNA-binding transcriptional activator NhaR] [protein_id=NP_414561.1] [location=18715..19620] [gbkey=CDS]', 'lcl|NC_000913.3_prot_NP_414571.1_30 [gene=rihC] [locus_tag=b0030] [db_xref=UniProtKB/Swiss-Prot:P22564] [protein=ribonucleoside hydrolase RihC] [protein_id=NP_414571.1] [location=27293..28207] [gbkey=CDS]', 'lcl|NC_000913.3_prot_NP_414544.1_3 [gene=thrB] [locus_tag=b0003] [db_xref=UniProtKB/Swiss-Prot:P00547] [protein=homoserine kinase] [protein_id=NP_414544.1] [location=2801..3733] [gbkey=CDS]', 'lcl|NC_000913.3_prot_NP_414566.1_25 [gene=ribF] [locus_tag=b0025] [db_xref=UniProtKB/Swiss-Prot:P0AG40] [protein=bifunctional riboflavin kinase/FMN adenylyltransferase] [protein_id=NP_414566.1] [location=21407..22348] [gbkey=CDS]', 'lcl|NC_000913.3_prot_NP_414584.1_42 [gene=fixB] [locus_tag=b0042] [db_xref=UniProtKB/Swiss-Prot:P31574] [protein=putative electron transfer flavoprotein FixB] [protein_id=NP_414584.1] [location=43188..44129] [gbkey=CDS]', 'lcl|NC_000913.3_prot_NP_414570.1_29 [gene=ispH] [locus_tag=b0029] [db_xref=UniProtKB/Swiss-Prot:P62623] [protein=4-hydroxy-3-methylbut-2-enyl diphosphate reductase] [protein_id=NP_414570.1] [location=26277..27227] [gbkey=CDS]', 'lcl|NC_000913.3_prot_NP_414549.1_8 [gene=talB] [locus_tag=b0008] [db_xref=UniProtKB/Swiss-Prot:P0A870] [protein=transaldolase B] [protein_id=NP_414549.1] [location=8238..9191] [gbkey=CDS]')}"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch = next(iter(data_loader))\n",
    "batch"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "FbTEKYrchErw",
   "metadata": {
    "id": "FbTEKYrchErw"
   },
   "source": [
    "## pLM Model\n",
    "\n",
    "Next, we load the `facebook/esm2_t33_650M_UR50D` from the Hugging Face Hub and send it directly to the device. It takes roughly 2.5 Gb of RAM in the GPU."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "a785e8f1",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 952,
     "referenced_widgets": [
      "0487396d0d624dc0bd3472932be58a3a",
      "f6a8998ac3d24472ac58b23e90f21e79",
      "8cab047017b045d39a7122f79d7a7d87",
      "05d77275e094464ea3b884f3a8c0876f",
      "a3e265ec835c4c7cb812f6a2d6977745",
      "99a88c8da83a4742b37db9ffca328130",
      "84fd016ed2d6443ab37b0b930e8705ad",
      "55c8d217b16e4ef4bc2ef99d8e7a6dd1",
      "1f3dd72cd8f946658fee2ee5d22c201b",
      "b68e1e5a0f604c2691c88ce9d9eb30e3",
      "f8212364ad3f49139051d7bc439261a4",
      "1ccc2da78be84e6bb99d26b00bd32462",
      "d7f9a9e4a44a40a7abeb590d603ef119",
      "419f46b51123422abd3cda2c9ea77e96",
      "70569fa9cbe34599864a3d020d78e615",
      "23b3857358f24680988782148a33fea9",
      "d45f800a9866476887c85a78802eb93f",
      "bc4d6e217ee341cfab4805f8df176c98",
      "90d15f6558d140d0a99fdcc4db3de261",
      "f478fa6bdc4148a8a04632275d0544f5",
      "1122799a523e4ce1b3e40275c1aeab8f",
      "70bbdb33431441aab26698552bc871ec"
     ]
    },
    "id": "a785e8f1",
    "outputId": "ab5561b2-81c2-45b7-9547-b6f1bdf26efe"
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0487396d0d624dc0bd3472932be58a3a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/724 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1ccc2da78be84e6bb99d26b00bd32462",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/2.61G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at facebook/esm2_t33_650M_UR50D were not used when initializing EsmForMaskedLM: ['esm.embeddings.position_embeddings.weight']\n",
      "- This IS expected if you are initializing EsmForMaskedLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing EsmForMaskedLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "EsmForMaskedLM(\n",
       "  (esm): EsmModel(\n",
       "    (embeddings): EsmEmbeddings(\n",
       "      (word_embeddings): Embedding(33, 1280, padding_idx=1)\n",
       "      (dropout): Dropout(p=0.0, inplace=False)\n",
       "    )\n",
       "    (encoder): EsmEncoder(\n",
       "      (layer): ModuleList(\n",
       "        (0-32): 33 x EsmLayer(\n",
       "          (attention): EsmAttention(\n",
       "            (self): EsmSelfAttention(\n",
       "              (query): Linear(in_features=1280, out_features=1280, bias=True)\n",
       "              (key): Linear(in_features=1280, out_features=1280, bias=True)\n",
       "              (value): Linear(in_features=1280, out_features=1280, bias=True)\n",
       "              (dropout): Dropout(p=0.0, inplace=False)\n",
       "              (rotary_embeddings): RotaryEmbedding()\n",
       "            )\n",
       "            (output): EsmSelfOutput(\n",
       "              (dense): Linear(in_features=1280, out_features=1280, bias=True)\n",
       "              (dropout): Dropout(p=0.0, inplace=False)\n",
       "            )\n",
       "            (LayerNorm): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)\n",
       "          )\n",
       "          (intermediate): EsmIntermediate(\n",
       "            (dense): Linear(in_features=1280, out_features=5120, bias=True)\n",
       "          )\n",
       "          (output): EsmOutput(\n",
       "            (dense): Linear(in_features=5120, out_features=1280, bias=True)\n",
       "            (dropout): Dropout(p=0.0, inplace=False)\n",
       "          )\n",
       "          (LayerNorm): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "      )\n",
       "      (emb_layer_norm_after): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)\n",
       "    )\n",
       "    (contact_head): EsmContactPredictionHead(\n",
       "      (regression): Linear(in_features=660, out_features=1, bias=True)\n",
       "      (activation): Sigmoid()\n",
       "    )\n",
       "  )\n",
       "  (lm_head): EsmLMHead(\n",
       "    (dense): Linear(in_features=1280, out_features=1280, bias=True)\n",
       "    (layer_norm): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)\n",
       "    (decoder): Linear(in_features=1280, out_features=33, bias=False)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Loads ESM model from HF Hub\n",
    "device = 'cuda:0' if torch.cuda.is_available() else 'cpu'\n",
    "esm_model = AutoModelForMaskedLM.from_pretrained(model_id, device_map=device)\n",
    "esm_model.eval()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "DeEqpppthHuK",
   "metadata": {
    "id": "DeEqpppthHuK"
   },
   "source": [
    "## pLM Embeddings\n",
    "\n",
    "To streamline the process of sending data to the ESM model, aggregating its outputs, and manage GPU memory to avoid OOM (out of memory) errors, we can use the `get_plm_embs()` function below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "f78070c7",
   "metadata": {
    "id": "f78070c7"
   },
   "outputs": [],
   "source": [
    "def get_plm_embs(esm_model, data_loader):\n",
    "    \"\"\"\n",
    "    Extracts protein language model (PLM) embeddings from the final hidden layer of the model\n",
    "    for each protein sequence in the data loader.\n",
    "\n",
    "    Args:\n",
    "        esm_model (nn.Module): A transformer-based model (e.g., ESM or similar) with output_hidden_states enabled.\n",
    "        data_loader (DataLoader): PyTorch DataLoader providing batches of protein input dictionaries\n",
    "                                  with keys: 'input_ids', 'attention_mask', 'prot_ids', 'prot_oris'.\n",
    "\n",
    "    Returns:\n",
    "        List[Tuple[str, str, torch.Tensor]]: A list of tuples where each tuple contains:\n",
    "            - prot_id (str): Protein identifier\n",
    "            - prot_ori (str): Original protein sequence string\n",
    "            - emb (torch.Tensor): Mean-pooled embedding over token-level representations (shape: [hidden_dim])\n",
    "    \"\"\"\n",
    "    plm_embs = []\n",
    "    torch.cuda.empty_cache()\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for batch in data_loader:\n",
    "            # Compute sequence lengths by summing attention mask and subtracting 2 special tokens (e.g., CLS, SEP)\n",
    "            lens = batch['attention_mask'].sum(dim=1) - 2\n",
    "\n",
    "            # Move input tensors to GPU\n",
    "            batch['input_ids'] = batch['input_ids'].to(device)\n",
    "            batch['attention_mask'] = batch['attention_mask'].to(device)\n",
    "\n",
    "            # Forward pass to get hidden states\n",
    "            output = esm_model(\n",
    "                input_ids=batch['input_ids'],\n",
    "                attention_mask=batch['attention_mask'],\n",
    "                output_hidden_states=True\n",
    "            )\n",
    "\n",
    "            # Get last hidden layer (shape: [batch_size, seq_len, hidden_dim])\n",
    "            states = output['hidden_states'][-1].detach().cpu()\n",
    "\n",
    "            del output\n",
    "            torch.cuda.empty_cache()\n",
    "\n",
    "            # Process each sequence in the batch\n",
    "            for pid, ori, seq_size, state in zip(batch['prot_ids'], batch['prot_oris'], lens, states):\n",
    "                truncate_len = min(MAX_LEN - 2, seq_size)  # Truncate to max allowed length\n",
    "                # Exclude special tokens and mean-pool across the valid tokens\n",
    "                pooled_emb = state[1:truncate_len + 1].mean(0).detach().cpu()\n",
    "                plm_embs.append((pid, ori, pooled_emb))\n",
    "\n",
    "    return plm_embs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "nyKqj-gweKrP",
   "metadata": {
    "id": "nyKqj-gweKrP"
   },
   "outputs": [],
   "source": [
    "plm_embs = get_plm_embs(esm_model, data_loader)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "342d7514",
   "metadata": {},
   "source": [
    "The function will return a list of tuples, one of each protein."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "7UvZW59veQ9F",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "7UvZW59veQ9F",
    "outputId": "c5622203-2485-4d43-b018-263706ca8109"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "60"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(plm_embs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8eb4e9bd",
   "metadata": {},
   "source": [
    "Each tuple contains the sequential ID assigned to the protein, its orientation, and its corresponding pLM embeddings returned by the ESM model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "22e777a9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0,\n",
       " '+',\n",
       " tensor([-0.0149,  0.0149,  0.0323,  ..., -0.0172,  0.0257,  0.0504],\n",
       "        dtype=torch.float16))"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "plm_embs[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "YWQ555c0hM_S",
   "metadata": {
    "id": "YWQ555c0hM_S"
   },
   "source": [
    "## Genome Language Modeling (gLM) Embeddings\n",
    "\n",
    "We're almost ready to retrieve contextual protein embeddings using the gLM model. But, first, we need to preprocess the ESM embeddings we obtained in the previous step.\n",
    "\n",
    "### Preprocessing Data\n",
    "\n",
    "There are two main preprocessing steps:\n",
    "\n",
    "- normalizing the embeddings (required for inference)\n",
    "- applying PCA to the embeddings (not required for inference, only for training/evaluation)\n",
    "\n",
    "The parameters for both transformations were already precomputed and are available for download. The custom dataset will load these parameters and apply them accordingly, as we'll see in the next section."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "6N-NuTuEdi8d",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "6N-NuTuEdi8d",
    "outputId": "9e898f07-291c-47cc-ec19-922647731607"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--2025-06-24 17:05:32--  https://github.com/dvgodoy/gLM/raw/refs/heads/main/inference/preproc/norm_factors.pt\n",
      "Resolving github.com (github.com)... 140.82.121.3\n",
      "Connecting to github.com (github.com)|140.82.121.3|:443... connected.\n",
      "HTTP request sent, awaiting response... 302 Found\n",
      "Location: https://raw.githubusercontent.com/dvgodoy/gLM/refs/heads/main/inference/preproc/norm_factors.pt [following]\n",
      "--2025-06-24 17:05:33--  https://raw.githubusercontent.com/dvgodoy/gLM/refs/heads/main/inference/preproc/norm_factors.pt\n",
      "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 2606:50c0:8001::154, 2606:50c0:8002::154, 2606:50c0:8000::154, ...\n",
      "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|2606:50c0:8001::154|:443... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 11702 (11K) [application/octet-stream]\n",
      "Saving to: ‘norm_factors.pt’\n",
      "\n",
      "norm_factors.pt     100%[===================>]  11.43K  --.-KB/s    in 0.001s  \n",
      "\n",
      "2025-06-24 17:05:33 (10.5 MB/s) - ‘norm_factors.pt’ saved [11702/11702]\n",
      "\n",
      "--2025-06-24 17:05:33--  https://github.com/dvgodoy/gLM/raw/refs/heads/main/inference/preproc/pca_parms.pt\n",
      "Resolving github.com (github.com)... 140.82.121.3\n",
      "Connecting to github.com (github.com)|140.82.121.3|:443... connected.\n",
      "HTTP request sent, awaiting response... 302 Found\n",
      "Location: https://raw.githubusercontent.com/dvgodoy/gLM/refs/heads/main/inference/preproc/pca_parms.pt [following]\n",
      "--2025-06-24 17:05:34--  https://raw.githubusercontent.com/dvgodoy/gLM/refs/heads/main/inference/preproc/pca_parms.pt\n",
      "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 2606:50c0:8001::154, 2606:50c0:8002::154, 2606:50c0:8000::154, ...\n",
      "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|2606:50c0:8001::154|:443... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 514146 (502K) [application/octet-stream]\n",
      "Saving to: ‘pca_parms.pt’\n",
      "\n",
      "pca_parms.pt        100%[===================>] 502.10K  2.98MB/s    in 0.2s    \n",
      "\n",
      "2025-06-24 17:05:35 (2.98 MB/s) - ‘pca_parms.pt’ saved [514146/514146]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "!wget https://github.com/dvgodoy/gLM/raw/refs/heads/main/inference/preproc/norm_factors.pt\n",
    "!wget https://github.com/dvgodoy/gLM/raw/refs/heads/main/inference/preproc/pca_parms.pt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "I10AAqKzhX01",
   "metadata": {
    "id": "I10AAqKzhX01"
   },
   "source": [
    "### Dataset\n",
    "\n",
    "We have implemented yet another dataset, `ContigDataset` that takes our former `FastaBatchedContigDataset`, the pLM embeddings, and the two files containing the preprocessing parameters. \n",
    "\n",
    "This a dataset of contigs, where each entry is a contig (like a sentence in a typical language model), and the contig is composed of a sequence of proteins (like a sentence is composed of words/tokens). \n",
    "\n",
    "The shape of the resulting mini-batch is `(N, L, D)` where N stands for the number of elements in the mini-batch (how many contigs), L stands for the sequence length (how many proteins in the contig), and D stands for the dimensionality of the pLM embeddings (1280).\n",
    "\n",
    "Moreover, just like typical LMs, if one contig is shorter (has fewer proteins) than other, it will be padded (-1 as protein ID) and the `attention_mask` will reflect that accordingly (containing a zero for the padded protein).\n",
    "\n",
    "The dataset will perform the following steps behind the scenes:\n",
    "\n",
    "- computing `input_embeds`\n",
    "    - normalize the pLM embeddings (1280 dimensions) using the provided normalization factors\n",
    "    - append an additional column containing either `+0.5` or `-0.5` according to the protein orientation, `+` or `-`, respectively (resulting in 1281 dimensions)\n",
    "- computing `labels` (we don't necessarily need them for inference)\n",
    "    - make a copy of the normalized pLM embeddings and apply the PCA transformation to it, thus reducing it to 99 dimensions only\n",
    "    - append an additional column containing either `+0.5` or `-0.5` according to the protein orientation, `+` or `-`, respectively (resulting in 100 dimensions)\n",
    "- pad shorter contigs with `-1` entries for protein IDs (`prot_ids` key)\n",
    "- update the `attention_mask` according to the padding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "601cfc05",
   "metadata": {
    "id": "601cfc05"
   },
   "outputs": [],
   "source": [
    "# Loads normalization parms to preprocess pLM embeddings\n",
    "# Applies PCA to pLM embeddings to get corresponding labels\n",
    "# Concatenates either +0.5 or -0.5 to each protein, both in\n",
    "# embeddings and labels, according to their origin, + or -\n",
    "nds = ContigDataset(cds, plm_embs, 'norm_factors.pt', 'pca_parms.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "16869066",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "16869066",
    "outputId": "218a2271-05ec-4c02-89fb-9c04fb4074c3"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'inputs_embeds': tensor([[[-7.4164e-01,  6.7392e-01,  7.5514e-01,  ...,  9.5282e-01,\n",
       "            1.4359e-01,  5.0000e-01],\n",
       "          [-1.3501e-01, -4.8856e-01,  9.2102e-01,  ...,  5.2716e-02,\n",
       "            4.1855e-01,  5.0000e-01],\n",
       "          [-1.0075e+00, -1.3698e+00,  7.2967e-01,  ...,  2.7863e-01,\n",
       "            8.1399e-01,  5.0000e-01],\n",
       "          ...,\n",
       "          [-7.0542e-01,  9.5284e-02,  1.6966e+00,  ..., -1.7093e-02,\n",
       "            2.1791e+00,  5.0000e-01],\n",
       "          [ 4.7144e-01, -5.7810e-01, -2.3907e-01,  ...,  7.3569e-01,\n",
       "            3.2090e+00,  5.0000e-01],\n",
       "          [ 6.7436e-01, -1.2447e+00,  1.9134e+00,  ..., -9.7436e-01,\n",
       "           -4.9839e-01,  5.0000e-01]],\n",
       " \n",
       "         [[ 1.6382e-01, -3.3953e-01,  1.0804e+00,  ...,  9.6821e-01,\n",
       "           -1.1294e+00,  5.0000e-01],\n",
       "          [ 6.9267e-04,  1.9632e-01, -2.1015e-01,  ...,  7.3679e-01,\n",
       "            2.8885e-01,  5.0000e-01],\n",
       "          [ 1.7924e-01,  1.6796e-01, -1.4275e+00,  ...,  7.8929e-01,\n",
       "            2.0567e+00,  5.0000e-01],\n",
       "          ...,\n",
       "          [ 5.9491e-02, -1.5318e+00, -9.4686e-03,  ...,  1.5410e+00,\n",
       "            1.4892e+00, -5.0000e-01],\n",
       "          [-4.6736e-01, -1.8713e-01,  5.7398e-01,  ...,  6.1978e-01,\n",
       "            5.7692e-01, -5.0000e-01],\n",
       "          [ 2.1043e-01, -3.4476e+00, -6.3886e-01,  ...,  1.0606e+00,\n",
       "            3.1779e+00, -5.0000e-01]]]),\n",
       " 'labels': tensor([[[ 1.5864, -0.0766,  0.3657,  ..., -1.5998, -4.4319,  0.5000],\n",
       "          [-0.9836,  0.8347,  0.8902,  ...,  0.6166, -0.7991,  0.5000],\n",
       "          [-0.9744,  1.0319,  1.1207,  ..., -0.3680,  0.1703,  0.5000],\n",
       "          ...,\n",
       "          [-0.5790,  0.6243,  0.3738,  ..., -1.8106,  0.0124,  0.5000],\n",
       "          [-1.0164,  0.9463,  1.0814,  ...,  0.3254,  1.0395,  0.5000],\n",
       "          [-0.6843,  1.0963,  0.5356,  ..., -1.3941,  0.0886,  0.5000]],\n",
       " \n",
       "         [[-0.8661,  0.6808,  0.5725,  ..., -0.8567, -1.3048,  0.5000],\n",
       "          [-0.6841,  0.3696,  0.5528,  ..., -2.7254,  0.3734,  0.5000],\n",
       "          [-0.7484,  0.6058,  0.4555,  ...,  0.7212, -0.1998,  0.5000],\n",
       "          ...,\n",
       "          [-0.7311,  1.0586,  1.1812,  ..., -1.5124, -0.3092, -0.5000],\n",
       "          [-0.8517,  0.7966,  0.5905,  ...,  0.0978, -0.0877, -0.5000],\n",
       "          [-1.5327,  2.0017,  2.3075,  ...,  1.0049, -0.0519, -0.5000]]]),\n",
       " 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "          1, 1, 1, 1, 1, 1],\n",
       "         [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "          1, 1, 1, 1, 1, 1]]),\n",
       " 'prot_ids': tensor([[ 0,  1,  2,  3,  4,  5,  6,  7,  8,  9, 10, 11, 12, 13, 14, 15, 16, 17,\n",
       "          18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29],\n",
       "         [30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47,\n",
       "          48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59]])}"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Uses custom collate function to include build mini-batches of N contigs\n",
    "# each contig being a sequence of L proteins\n",
    "# each protein represented by D dimensions (1280 for embeddings, 100 for PCA-transformed labels)\n",
    "# (N, L, D) shapes\n",
    "# If some contigs are shorter than others, the attention masks are updated to reflect padding\n",
    "# and the padded protein IDs will be -1\n",
    "ndl = DataLoader(nds, batch_size=2, collate_fn=contig_collate_fn)\n",
    "\n",
    "batch = next(iter(ndl))\n",
    "batch"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1JHZzDcmhe78",
   "metadata": {
    "id": "1JHZzDcmhe78"
   },
   "source": [
    "### gLM Model\n",
    "\n",
    "To load the gLM model, we must first build its architecture, which is based on the RoBERTa model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e3dabc1",
   "metadata": {},
   "outputs": [],
   "source": [
    "HIDDEN_SIZE = 1280\n",
    "HALF = True\n",
    "EMB_DIM = 1281\n",
    "NUM_PC_LABEL = 100\n",
    "\n",
    "num_pred = 4\n",
    "max_seq_length = 30\n",
    "num_attention_heads = 10\n",
    "num_hidden_layers= 19\n",
    "pos_emb = \"relative_key_query\"\n",
    "pred_probs = True\n",
    "\n",
    "# Creates RoBERTa configuration to load gLM\n",
    "config = RobertaConfig(\n",
    "    vocab_size = 30522,\n",
    "    max_position_embedding = max_seq_length,\n",
    "    hidden_size = HIDDEN_SIZE,\n",
    "    num_attention_heads = num_attention_heads,\n",
    "    type_vocab_size = 1,\n",
    "    tie_word_embeddings = False,\n",
    "    num_hidden_layers = num_hidden_layers,\n",
    "    num_pc = NUM_PC_LABEL,\n",
    "    num_pred = num_pred,\n",
    "    predict_probs = pred_probs,\n",
    "    emb_dim = EMB_DIM,\n",
    "    output_attentions = True,\n",
    "    output_hidden_states = True,\n",
    "    position_embedding_type = pos_emb,\n",
    "    attn_implementation = \"eager\"\n",
    ")\n",
    "\n",
    "# Loads configuration to build architecture\n",
    "model =  gLM(config)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b5ce9f8",
   "metadata": {},
   "source": [
    "Now, we need to download its pretrained weights:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "0fca79af",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "0fca79af",
    "outputId": "21deed3a-db27-4e73-d9cc-99a55f3707dd"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mkdir: cannot create directory ‘model’: File exists\n",
      "--2025-06-24 15:12:10--  https://zenodo.org/records/7855545/files/glm.bin\n",
      "Resolving zenodo.org (zenodo.org)... 188.185.43.25, 188.185.48.194, 188.185.45.92, ...\n",
      "Connecting to zenodo.org (zenodo.org)|188.185.43.25|:443... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 3819298935 (3.6G) [application/octet-stream]\n",
      "Saving to: ‘./model/glm.bin’\n",
      "\n",
      "./model/glm.bin     100%[===================>]   3.56G  2.93MB/s    in 18m 27s \n",
      "\n",
      "2025-06-24 15:30:38 (3.29 MB/s) - ‘./model/glm.bin’ saved [3819298935/3819298935]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Downloads the pretrained gLM model\n",
    "# This download can be painfully slow at times!\n",
    "!mkdir model\n",
    "!wget https://zenodo.org/records/7855545/files/glm.bin --output-document=./model/glm.bin"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8cfba517",
   "metadata": {},
   "source": [
    "Finally, we load the weights into the model and set it to evaluation mode."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "1ff8662a",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "1ff8662a",
    "outputId": "363b1518-6f41-4840-ebc8-1419a2d6f733"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "gLM(\n",
       "  (embeddings): RobertaEmbeddings(\n",
       "    (word_embeddings): Embedding(30522, 1280, padding_idx=1)\n",
       "    (position_embeddings): Embedding(512, 1280, padding_idx=1)\n",
       "    (token_type_embeddings): Embedding(1, 1280)\n",
       "    (LayerNorm): LayerNorm((1280,), eps=1e-12, elementwise_affine=True)\n",
       "    (dropout): Dropout(p=0.1, inplace=False)\n",
       "  )\n",
       "  (encoder): RobertaEncoder(\n",
       "    (layer): ModuleList(\n",
       "      (0-18): 19 x RobertaLayer(\n",
       "        (attention): RobertaAttention(\n",
       "          (self): RobertaSelfAttention(\n",
       "            (query): Linear(in_features=1280, out_features=1280, bias=True)\n",
       "            (key): Linear(in_features=1280, out_features=1280, bias=True)\n",
       "            (value): Linear(in_features=1280, out_features=1280, bias=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "            (distance_embedding): Embedding(1023, 128)\n",
       "          )\n",
       "          (output): RobertaSelfOutput(\n",
       "            (dense): Linear(in_features=1280, out_features=1280, bias=True)\n",
       "            (LayerNorm): LayerNorm((1280,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (intermediate): RobertaIntermediate(\n",
       "          (dense): Linear(in_features=1280, out_features=3072, bias=True)\n",
       "          (intermediate_act_fn): GELUActivation()\n",
       "        )\n",
       "        (output): RobertaOutput(\n",
       "          (dense): Linear(in_features=3072, out_features=1280, bias=True)\n",
       "          (LayerNorm): LayerNorm((1280,), eps=1e-12, elementwise_affine=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (pooler): RobertaPooler(\n",
       "    (dense): Linear(in_features=1280, out_features=1280, bias=True)\n",
       "    (activation): Tanh()\n",
       "  )\n",
       "  (roberta): gLM_base(\n",
       "    (embeddings): RobertaEmbeddings(\n",
       "      (word_embeddings): Embedding(30522, 1280, padding_idx=1)\n",
       "      (position_embeddings): Embedding(512, 1280, padding_idx=1)\n",
       "      (token_type_embeddings): Embedding(1, 1280)\n",
       "      (LayerNorm): LayerNorm((1280,), eps=1e-12, elementwise_affine=True)\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (encoder): RobertaEncoder(\n",
       "      (layer): ModuleList(\n",
       "        (0-18): 19 x RobertaLayer(\n",
       "          (attention): RobertaAttention(\n",
       "            (self): RobertaSelfAttention(\n",
       "              (query): Linear(in_features=1280, out_features=1280, bias=True)\n",
       "              (key): Linear(in_features=1280, out_features=1280, bias=True)\n",
       "              (value): Linear(in_features=1280, out_features=1280, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "              (distance_embedding): Embedding(1023, 128)\n",
       "            )\n",
       "            (output): RobertaSelfOutput(\n",
       "              (dense): Linear(in_features=1280, out_features=1280, bias=True)\n",
       "              (LayerNorm): LayerNorm((1280,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): RobertaIntermediate(\n",
       "            (dense): Linear(in_features=1280, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): RobertaOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=1280, bias=True)\n",
       "            (LayerNorm): LayerNorm((1280,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (pooler): RobertaPooler(\n",
       "      (dense): Linear(in_features=1280, out_features=1280, bias=True)\n",
       "      (activation): Tanh()\n",
       "    )\n",
       "    (roberta): RobertaModel(\n",
       "      (embeddings): RobertaEmbeddings(\n",
       "        (word_embeddings): Embedding(30522, 1280, padding_idx=1)\n",
       "        (position_embeddings): Embedding(512, 1280, padding_idx=1)\n",
       "        (token_type_embeddings): Embedding(1, 1280)\n",
       "        (LayerNorm): LayerNorm((1280,), eps=1e-12, elementwise_affine=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (encoder): RobertaEncoder(\n",
       "        (layer): ModuleList(\n",
       "          (0-18): 19 x RobertaLayer(\n",
       "            (attention): RobertaAttention(\n",
       "              (self): RobertaSelfAttention(\n",
       "                (query): Linear(in_features=1280, out_features=1280, bias=True)\n",
       "                (key): Linear(in_features=1280, out_features=1280, bias=True)\n",
       "                (value): Linear(in_features=1280, out_features=1280, bias=True)\n",
       "                (dropout): Dropout(p=0.1, inplace=False)\n",
       "                (distance_embedding): Embedding(1023, 128)\n",
       "              )\n",
       "              (output): RobertaSelfOutput(\n",
       "                (dense): Linear(in_features=1280, out_features=1280, bias=True)\n",
       "                (LayerNorm): LayerNorm((1280,), eps=1e-12, elementwise_affine=True)\n",
       "                (dropout): Dropout(p=0.1, inplace=False)\n",
       "              )\n",
       "            )\n",
       "            (intermediate): RobertaIntermediate(\n",
       "              (dense): Linear(in_features=1280, out_features=3072, bias=True)\n",
       "              (intermediate_act_fn): GELUActivation()\n",
       "            )\n",
       "            (output): RobertaOutput(\n",
       "              (dense): Linear(in_features=3072, out_features=1280, bias=True)\n",
       "              (LayerNorm): LayerNorm((1280,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (lm_head): gLMMultiHead(\n",
       "      (dense): Linear(in_features=1280, out_features=400, bias=True)\n",
       "      (dense_prob): Linear(in_features=1280, out_features=4, bias=True)\n",
       "    )\n",
       "    (contact_head): ContactPredictionHead(\n",
       "      (regression): Linear(in_features=190, out_features=1, bias=True)\n",
       "    )\n",
       "  )\n",
       "  (dense): Linear(in_features=1281, out_features=1280, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Loads saved weights\n",
    "model_path = './model/glm.bin'\n",
    "model.load_state_dict(torch.load(model_path, map_location=device),strict=False)\n",
    "model.to(device)\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3173797d",
   "metadata": {},
   "source": [
    "The model has two head, `lm_head` and `contact_head`, for predicting the label embeddings and contacts, respectively. Notice that the `lm_head` is an instance of the `gLMMultiHead`, which produces 400 output features (4 heads for producing 4 candidates, each having 100 dimensions from the PCA-transformed labels with an extra column), and 4 logits for the candidates' corresponding probabilities."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "Ptaoj3CEhimk",
   "metadata": {
    "id": "Ptaoj3CEhimk"
   },
   "source": [
    "### gLM Embeddings\n",
    "\n",
    "To streamline the process of sending data to our gLM model, aggregating its outputs, and manage GPU memory to avoid OOM (out of memory) errors, we can use the `get_glm_embs()` function below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "15575d56",
   "metadata": {
    "id": "15575d56"
   },
   "outputs": [],
   "source": [
    "def get_glm_embs(model, data_loader, device, half_precision=True):\n",
    "    \"\"\"\n",
    "    Extracts embeddings and prediction outputs from a gLM model over a given dataset.\n",
    "\n",
    "    Args:\n",
    "        model (nn.Module): A gLM model that returns a dictionary of outputs including\n",
    "                           'logits_all_preds', 'probs', 'last_hidden_state', and 'contacts'.\n",
    "        data_loader (DataLoader): PyTorch DataLoader yielding batches containing:\n",
    "                                  'inputs_embeds', 'labels', and 'attention_mask'.\n",
    "        device (torch.device): The CUDA or CPU device to run the model on.\n",
    "        half_precision (bool, optional): Whether to use automatic mixed precision (AMP) inference on GPU.\n",
    "                                         Defaults to True, but is disabled if CUDA is unavailable.\n",
    "\n",
    "    Returns:\n",
    "        Tuple[Dict[str, torch.Tensor], Dict[str, torch.Tensor]]:\n",
    "            - all_batches: Dictionary with concatenated input batches (e.g., inputs_embeds, labels, etc.).\n",
    "            - all_outputs: Dictionary with concatenated model outputs across all batches.\n",
    "    \"\"\"\n",
    "    if not torch.cuda.is_available():\n",
    "        half_precision = False  # Disable AMP if CUDA isn't available\n",
    "\n",
    "    torch.cuda.empty_cache()\n",
    "    scaler = None  # AMP gradient scaling\n",
    "\n",
    "    with torch.no_grad():\n",
    "        all_outputs = {}  # Stores outputs across all batches\n",
    "        all_batches = {}  # Stores inputs across all batches\n",
    "\n",
    "        for batch in data_loader:\n",
    "            # Move batch components to the target device\n",
    "            batch['inputs_embeds'] = batch['inputs_embeds'].to(device)\n",
    "            batch['labels'] = batch['labels'].to(device)\n",
    "            batch['attention_mask'] = batch['attention_mask'].to(device)\n",
    "\n",
    "            # Run inference with AMP if half_precision is enabled\n",
    "            if half_precision:\n",
    "                scaler = torch.cuda.amp.GradScaler()\n",
    "                with torch.amp.autocast(device_type='cuda', dtype=torch.float16):\n",
    "                    output = model(\n",
    "                        inputs_embeds=batch['inputs_embeds'],\n",
    "                        labels=batch['labels'],\n",
    "                        attention_mask=batch['attention_mask']\n",
    "                    )\n",
    "            else:\n",
    "                output = model(\n",
    "                    inputs_embeds=batch['inputs_embeds'],\n",
    "                    labels=batch['labels'],\n",
    "                    attention_mask=batch['attention_mask']\n",
    "                )\n",
    "\n",
    "            # Accumulate model outputs (detach and move to CPU to free GPU memory)\n",
    "            for k in ['logits_all_preds', 'probs', 'last_hidden_state', 'contacts']:\n",
    "                current = all_outputs.get(k, None)\n",
    "                if current is None:\n",
    "                    all_outputs[k] = output[k].detach().cpu()\n",
    "                else:\n",
    "                    all_outputs[k] = torch.cat([all_outputs[k], output[k].detach().cpu()], dim=0)\n",
    "\n",
    "            del output\n",
    "\n",
    "            # Accumulate input batches as well\n",
    "            for k in batch.keys():\n",
    "                current = all_batches.get(k, None)\n",
    "                if current is None:\n",
    "                    all_batches[k] = batch[k].detach().cpu()\n",
    "                else:\n",
    "                    all_batches[k] = torch.cat([all_batches[k], batch[k].detach().cpu()], dim=0)\n",
    "\n",
    "            torch.cuda.empty_cache()  # Prevent GPU memory buildup\n",
    "\n",
    "    return all_batches, all_outputs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bedb041e",
   "metadata": {},
   "source": [
    "The function above will return plenty of information:\n",
    "\n",
    "- from the batches:\n",
    "    - input embeddings (normalized pLM embeddings with an appended column)\n",
    "    - label embeddings (PCA-transformed normalized pLM embeddings with an appended column)\n",
    "    - protein IDs\n",
    "- from the outputs:\n",
    "    - contextual gLM embeddings (from the last hidden state in the gLM model)\n",
    "    - contact prediction based on embeddings and attention scores\n",
    "    - a set of 4 output embeddings (predicted label embeddings, `logits_all_preds`)\n",
    "    - a set of 4 probabilities for their corresponding predictions (`probs`)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "blNVAWB4lxsg",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "blNVAWB4lxsg",
    "outputId": "bd492fed-d64c-4c51-abc3-c89a4b4a4be9"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['logits_all_preds', 'probs', 'last_hidden_state', 'contacts'])"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Retrieve gLM contextual embeddings\n",
    "all_batches, all_outputs = get_glm_embs(model, ndl, device, HALF)\n",
    "all_outputs.keys()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "VC0pimK-m7O4",
   "metadata": {
    "id": "VC0pimK-m7O4"
   },
   "source": [
    "### Saving\n",
    "\n",
    "Finally, we can use the `save_results()` function to organize the outputs, stack them (removing the batch dimension), and save them into three files:\n",
    "- one containing all results (_results.pt)\n",
    "- one containing only gLM embeddings (_glm_embs.pt)\n",
    "- one containing attention scores/contacts (_attention.pt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "kp9Mb0Dsm34z",
   "metadata": {
    "id": "kp9Mb0Dsm34z"
   },
   "outputs": [],
   "source": [
    "# Saves results to three files:\n",
    "\n",
    "save_results(all_batches, all_outputs, './results', 'test')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "4pqpBUX0GzWW",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "4pqpBUX0GzWW",
    "outputId": "9f76448e-e076-45c4-9509-d0046e12d7fa"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0,\n",
       " array([ 0.3616 ,  0.0725 , -0.3357 , ..., -0.2272 ,  0.02904,  0.064  ],\n",
       "       dtype=float16))"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "glm_embs = torch.load('./results/test_glm_embs.pt', weights_only=False)\n",
    "glm_embs[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "HOwrTbXyiXMI",
   "metadata": {
    "id": "HOwrTbXyiXMI"
   },
   "source": [
    "You should see the following result:\n",
    "\n",
    "`(0, array([ 0.3616 ,  0.0725 , -0.3357 , ..., -0.2272 ,  0.02904,  0.064  ], dtype=float16))`"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "0487396d0d624dc0bd3472932be58a3a": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_f6a8998ac3d24472ac58b23e90f21e79",
       "IPY_MODEL_8cab047017b045d39a7122f79d7a7d87",
       "IPY_MODEL_05d77275e094464ea3b884f3a8c0876f"
      ],
      "layout": "IPY_MODEL_a3e265ec835c4c7cb812f6a2d6977745"
     }
    },
    "05d77275e094464ea3b884f3a8c0876f": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_b68e1e5a0f604c2691c88ce9d9eb30e3",
      "placeholder": "​",
      "style": "IPY_MODEL_f8212364ad3f49139051d7bc439261a4",
      "value": " 724/724 [00:00&lt;00:00, 77.5kB/s]"
     }
    },
    "1122799a523e4ce1b3e40275c1aeab8f": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "1ccc2da78be84e6bb99d26b00bd32462": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_d7f9a9e4a44a40a7abeb590d603ef119",
       "IPY_MODEL_419f46b51123422abd3cda2c9ea77e96",
       "IPY_MODEL_70569fa9cbe34599864a3d020d78e615"
      ],
      "layout": "IPY_MODEL_23b3857358f24680988782148a33fea9"
     }
    },
    "1f3dd72cd8f946658fee2ee5d22c201b": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "23b3857358f24680988782148a33fea9": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "419f46b51123422abd3cda2c9ea77e96": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_90d15f6558d140d0a99fdcc4db3de261",
      "max": 2609506392,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_f478fa6bdc4148a8a04632275d0544f5",
      "value": 2609506392
     }
    },
    "55c8d217b16e4ef4bc2ef99d8e7a6dd1": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "70569fa9cbe34599864a3d020d78e615": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_1122799a523e4ce1b3e40275c1aeab8f",
      "placeholder": "​",
      "style": "IPY_MODEL_70bbdb33431441aab26698552bc871ec",
      "value": " 2.61G/2.61G [00:35&lt;00:00, 157MB/s]"
     }
    },
    "70bbdb33431441aab26698552bc871ec": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "84fd016ed2d6443ab37b0b930e8705ad": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "8cab047017b045d39a7122f79d7a7d87": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_55c8d217b16e4ef4bc2ef99d8e7a6dd1",
      "max": 724,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_1f3dd72cd8f946658fee2ee5d22c201b",
      "value": 724
     }
    },
    "90d15f6558d140d0a99fdcc4db3de261": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "99a88c8da83a4742b37db9ffca328130": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "a3e265ec835c4c7cb812f6a2d6977745": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "b68e1e5a0f604c2691c88ce9d9eb30e3": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "bc4d6e217ee341cfab4805f8df176c98": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "d45f800a9866476887c85a78802eb93f": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "d7f9a9e4a44a40a7abeb590d603ef119": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_d45f800a9866476887c85a78802eb93f",
      "placeholder": "​",
      "style": "IPY_MODEL_bc4d6e217ee341cfab4805f8df176c98",
      "value": "model.safetensors: 100%"
     }
    },
    "f478fa6bdc4148a8a04632275d0544f5": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "f6a8998ac3d24472ac58b23e90f21e79": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_99a88c8da83a4742b37db9ffca328130",
      "placeholder": "​",
      "style": "IPY_MODEL_84fd016ed2d6443ab37b0b930e8705ad",
      "value": "config.json: 100%"
     }
    },
    "f8212364ad3f49139051d7bc439261a4": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
